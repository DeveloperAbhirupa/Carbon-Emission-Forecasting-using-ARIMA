{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Table of Contents\n",
    "1.\tIntroduction\n",
    "2.\tTime series dataset\n",
    "3.\tImport libraries\n",
    "4.\tTime series dataset retrieving and visualization\n",
    "5.  Natural gas CO2 emission analysis\n",
    "    \n",
    "    5.1\tTest stationary\n",
    "        5.1.1 Graphically test stationary  \n",
    "        5.1.2 Test stationarity using Dickey-Fuller test\n",
    "        5.1.3 Transform the dataset to stationary\n",
    "6.\tFind optimal parameters and build SARIMA model\n",
    "7.\tValidating prediction\n",
    "8.  Forecasting \n",
    "9.\tConclusion\n",
    "\n",
    "# 1) Introduction\n",
    "Time series is a collection of data points that are collected at constant time intervals. It is a dynamic or time dependent problem with or without increasing or decreasing trend, seasonality. Time series modeling is a powerful method to describe and extract information from time-based data and help to make informed decisions about future outcomes.\n",
    "\n",
    "This notebook explores how to retrieve csv times series dataset, visualizing time series dataset, how to transform dataset into times series, testing if the time series is stationary or not using graphical and Dickey-Fuller test statistic methods, how to transform time series to stationary, how to find optimal parameters to build seasonal Autoregressive Integrated Moving Average (SARIMA) model using grid search method, diagnosing time series prediction, validating the predictive power, forecasting 10 year future CO2 emission from power generation using natural gas.,  \n",
    "    \n",
    "To complete this time series analysis, I use the following articles that cover the fundamental concepts about time series modeling:\n",
    "\n",
    " [Time series forecasting with codes in Python](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/).\n",
    "\n",
    "[Statistical forecasting: notes on regression and time series analysis](http://people.duke.edu/~rnau/411home.htm).\n",
    "  \n",
    "  [Time Series Forecasting: Creating a seasonal ARIMA model with Python ](http://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/).\n",
    "  \n",
    " [Forecast a time series with ARIMA in Python](https://datascience.ibm.com/exchange/public/entry/view/815137c868b916821dec777bdc23013c).\n",
    " \n",
    " [A Guide to Time Series Forecasting with ARIMA in Python 3](https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-arima-in-python-3)\n",
    "\n",
    "\n",
    "# 2) Time series dataset\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "\n",
    "\n",
    "I use a public dataset of monthly carbon dioxide emissions from electricity generation available at the Energy Information Administration and Jason McNeill. The dataset includes CO2 emissions from each energy resource starting January 1973 to July 2016 for reference click [here](https://www.kaggle.com/txtrouble/carbon-emissions/data). \n",
    "\n",
    "# 3)\tImport libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 16\n",
    "\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "\n",
    "# 4)\tTime series dataset retrieving and visualization\n",
    "\n",
    "First, in the following cells, we will retrive the monthly CO2 emissions dataset then we will visualize the dataset to decide the type of model we will use to model and analyse our time series (ts).\n",
    "\n",
    "## 4.1\tTime series dataset retrieving\n",
    "\n",
    "df = pd.read_csv(\"../input/MER_T12_06.csv\")\n",
    "df.head()\n",
    "\n",
    "df.info()\n",
    "\n",
    "The dataset has 6 columns where 2 of them are integer data type and 4 objects and 5096 observations. The above dataset retriving method only retrives the dataset as a dataframe that is not as a time series dataset. To read the dataset as a time series, we have to pass special arguments to the read_csv command as given below.\n",
    "\n",
    "dateparse = lambda x: pd.to_datetime(x, format='%Y%m', errors = 'coerce')\n",
    "df = pd.read_csv(\"Data/MER_T12_06.csv\", parse_dates=['YYYYMM'], index_col='YYYYMM', date_parser=dateparse) \n",
    "df.head()\n",
    "\n",
    "The arguments can be explained:\n",
    "- parse_dates: This is a key to identify the date time column. Example, the column name is ‘YYYYMM’.\n",
    "- index_col: This is a key that forces pandas to use the date time column as index.\n",
    "- date_parser: Converts an input string into datetime variable.\n",
    "\n",
    "df.head(15)\n",
    "\n",
    "Total sum of CO2 emission from each energy group for every year is given as an observation that can be viewed in the NaT row. So, let us first identify and drop the non datetimeindex rows and also use ts to refere the time series dataset instead of the dataframe df. First, let us convert the index to datetime, coerce errors, and filter NaT\n",
    "\n",
    "ts = df[pd.Series(pd.to_datetime(df.index, errors='coerce')).notnull().values]\n",
    "ts.head(15)\n",
    "\n",
    "ts.dtypes\n",
    "\n",
    "As we can see from the ts data type, the emission value is represented as an object. Let us first convert the emision value into numeric value as follows\n",
    "\n",
    "#ss = ts.copy(deep=True)\n",
    "ts['Value'] = pd.to_numeric(ts['Value'] , errors='coerce')\n",
    "ts.head()\n",
    "\n",
    "ts.info()\n",
    "\n",
    "4323 observations have emissions value and therefore, we need to drop the empty rows emissions value. \n",
    "\n",
    "ts.dropna(inplace = True)\n",
    "\n",
    "## 4.2\tTime series dataset visualization\n",
    "\n",
    "The dataset has 8 energy sources of CO2 emission. In the following cell, we will group the CO2 Emission dataset based on the type of energy source.\n",
    "\n",
    "Energy_sources = ts.groupby('Description')\n",
    "Energy_sources.head()\n",
    "\n",
    "The CO2 emission time series dataset is ploted to visualize the dependency of the emission in the power generation with time. \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for desc, group in Energy_sources:\n",
    "    group.plot(x = group.index, y='Value', label=desc,ax = ax, title='Carbon Emissions per Energy Source', fontsize = 20)\n",
    "    ax.set_xlabel('Time(Monthly)')\n",
    "    ax.set_ylabel('Carbon Emissions in MMT')\n",
    "    ax.xaxis.label.set_size(20)\n",
    "    ax.yaxis.label.set_size(20)\n",
    "    ax.legend(fontsize = 16)\n",
    "\n",
    "Individually, we can visualize the trend and seasonality effect on CO2 emission from each energy source. For example, the CO2 emission from coal shows a trend of increment from 1973 to 2006 and then declines till 2016. \n",
    "\n",
    "fig, axes = plt.subplots(3,3, figsize = (30, 20))\n",
    "for (desc, group), ax in zip(Energy_sources, axes.flatten()):\n",
    "    group.plot(x = group.index, y='Value',ax = ax, title=desc, fontsize = 18)\n",
    "    ax.set_xlabel('Time(Monthly)')\n",
    "    ax.set_ylabel('Carbon Emissions in MMT')\n",
    "    ax.xaxis.label.set_size(18)\n",
    "    ax.yaxis.label.set_size(18)\n",
    "\n",
    "In recent years, the natural gas consumption has been increasing. However, the use of coal for power generation has been declining. The plots of CO2 emissions from coal and natural gas show this trend, while declining the CO2 contribution from coal, there is an increment in the contribution of CO2 emission from natural gas.\n",
    "\n",
    "   ## 4.3\tBar chart of CO2 Emissions per energy source\n",
    "\n",
    "CO2_per_source = ts.groupby('Description')['Value'].sum().sort_values()\n",
    "\n",
    "# I want to use shorter descriptions for the energy sources\n",
    "CO2_per_source.index\n",
    "\n",
    "cols = ['Geothermal Energy', 'Non-Biomass Waste', 'Petroleum Coke','Distillate Fuel ',\n",
    "        'Residual Fuel Oil', 'Petroleum', 'Natural Gas', 'Coal', 'Total Emissions']\n",
    "\n",
    "fig = plt.figure(figsize = (16,9))\n",
    "x_label = cols\n",
    "x_tick = np.arange(len(cols))\n",
    "plt.bar(x_tick, CO2_per_source, align = 'center', alpha = 0.5)\n",
    "fig.suptitle(\"CO2 Emissions by Electric Power Sector\", fontsize= 25)\n",
    "plt.xticks(x_tick, x_label, rotation = 70, fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlabel('Carbon Emissions in MMT', fontsize = 20)\n",
    "plt.show()\n",
    "\n",
    "**From the bar chart, we can see that the contribution of coal to the total CO2 emission is significant followed by natural gas. **\n",
    "\n",
    "# 5) Natural gas CO2 emission analysis\n",
    "\n",
    "For developing the time series model and make forcasting, I will use the natural gas CO2 emission from the electirical power generetion. First, let us slice this data from the ts as follows:  \n",
    "\n",
    "Emissions = ts.iloc[:,1:]   # Monthly total emissions (mte)\n",
    "Emissions= Emissions.groupby(['Description', pd.TimeGrouper('M')])['Value'].sum().unstack(level = 0)\n",
    "mte = Emissions['Natural Gas Electric Power Sector CO2 Emissions'] # monthly total emissions (mte)\n",
    "mte.head()\n",
    "\n",
    "mte.tail()\n",
    "\n",
    "## 5.1 Test Stationary\n",
    "\n",
    "The first thing we need to do is producing a plot of our time series dataset. From the plot, we will get an idea about the overall trend and seasonality of the series. Then, we will use a statistical method to assess the trend and seasonality of the dataset.  After trend and seasonality are assessed if they are present in the dataset, they will be removed from the series to transform the nonstationary dataset into stationary and the residuals are further analyzed.\n",
    "   \n",
    "A short summary about stationarity from Wikipedia: A stationary process is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance, if they are present, also do not change over time. \n",
    "\n",
    "Stationarity is an assumption underlying many statistical procedures used in time series analysis, non-stationary data is often transformed to become stationary. The most common cause of violation of stationarity is a trend in the mean, which can be due to either the presence of a unit root or of a deterministic trend. If the nonstationarity is caused by the presence of unit root, stochastic shocks have permanent effects and the process is not mean-reverting. However, if it is caused by a deterministic trend, the process is called a trend stationary process, and stochastic shocks have only transitory effects after which the variable tends toward a deterministically evolving mean.\n",
    "    \n",
    " A trend stationary process is not strictly stationary, but can easily be transformed into a stationary process by removing the underlying trend, which is solely a function of time. Similarly, processes with one or more unit roots can be made stationary through differencing. An important type of non-stationary process that does not include a trend-like behavior is a cyclostationary process, which is a stochastic process that varies cyclically with time.\n",
    "\n",
    "Note: Given two jointly distributed random variables X and Y, the conditional probability distribution of Y given X is the probability distribution of Y when X is known to be a particular value.\n",
    "\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "\n",
    "## 5.1.1 Graphycally test stationary\n",
    "\n",
    "plt.plot(mte)\n",
    "\n",
    "From the figures, it is evident that there is a trend in the CO2 emission dataset with seasonal variation. So, we can infer a concluding remark that the dataset is not stationary.\n",
    "\n",
    "## 5.1.2 Test  stationary using Dickey-Fuller\n",
    "\n",
    "A formal way of testing stationarity of a dataset is using plotting the moving average or moving variance and see if the series mean and variance varies with time. This approach will be handled by the TestStationaryPlot() method. The second way to test stationarity is to use the statistical test (the Dickey-Fuller Test). The null hypothesis for the test is that the time series is non-stationary. The test results compare a Test Statistic and Critical Values (cutoff value) at different confidence levels. If the ‘Test Statistic’ is less than the ‘Critical Value’, we can reject the null hypothesis and say that the series is stationary. This technique will be handled by the TestStationaryAdfuller( ) method given below.\n",
    "\n",
    "\n",
    "def TestStationaryPlot(ts):\n",
    "    rol_mean = ts.rolling(window = 12, center = False).mean()\n",
    "    rol_std = ts.rolling(window = 12, center = False).std()\n",
    "    \n",
    "    plt.plot(ts, color = 'blue',label = 'Original Data')\n",
    "    plt.plot(rol_mean, color = 'red', label = 'Rolling Mean')\n",
    "    plt.plot(rol_std, color ='black', label = 'Rolling Std')\n",
    "    plt.xticks(fontsize = 25)\n",
    "    plt.yticks(fontsize = 25)\n",
    "    \n",
    "    plt.xlabel('Time in Years', fontsize = 25)\n",
    "    plt.ylabel('Total Emissions', fontsize = 25)\n",
    "    plt.legend(loc='best', fontsize = 25)\n",
    "    plt.title('Rolling Mean & Standard Deviation', fontsize = 25)\n",
    "    plt.show(block= True)\n",
    "\n",
    "def TestStationaryAdfuller(ts, cutoff = 0.01):\n",
    "    ts_test = adfuller(ts, autolag = 'AIC')\n",
    "    ts_test_output = pd.Series(ts_test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    \n",
    "    for key,value in ts_test[4].items():\n",
    "        ts_test_output['Critical Value (%s)'%key] = value\n",
    "    print(ts_test_output)\n",
    "    \n",
    "    if ts_test[1] <= cutoff:\n",
    "        print(\"Strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n",
    "        \n",
    "\n",
    "### Testing the monthly emissions time series\n",
    "\n",
    "TestStationaryPlot(mte)\n",
    "\n",
    "TestStationaryAdfuller(mte)\n",
    "\n",
    "The emissions mean and the variation in standard deviation (black line) clearly vary with time. This shows that the series has a trend. So, it is not a stationary. Also, the Test Statistic is greater than the critical values with 90%, 95% and 99% confidence levels. Hence, no evidence to reject the null hypothesis. Therefore the series is nonstationary. \n",
    "\n",
    "## 5.1.3 Transform the dataset to stationary\n",
    "\n",
    "The most common techniques used to estimate or model trend and then remove it from the time series are \n",
    "- Aggregation – taking average for a time period like monthly/weekly average\n",
    "- Smoothing – taking rolling averages\n",
    "- Polynomial Fitting – fit a regression model\n",
    "\n",
    "## A). Moving average\n",
    "\n",
    "In this technique, we take average of ‘k’ consecutive values depending on the frequency of time series (in this case 12 monthes per year). Here, we will take the average over the past 1 year.\n",
    "\n",
    "moving_avg = mte.rolling(12).mean()\n",
    "plt.plot(mte)\n",
    "plt.plot(moving_avg, color='red')\n",
    "plt.xticks(fontsize = 25)\n",
    "plt.yticks(fontsize = 25)\n",
    "plt.xlabel('Time (years)', fontsize = 25)\n",
    "plt.ylabel('CO2 Emission (MMT)', fontsize = 25)\n",
    "plt.title('CO2 emission from electric power generation', fontsize = 25)\n",
    "plt.show()\n",
    "\n",
    "The red line shows the rolling mean. Subtract the moving average from the original series. Note that since we are taking average of last 12 values, rolling mean is not defined for first 11 values. \n",
    "\n",
    "mte_moving_avg_diff = mte - moving_avg\n",
    "mte_moving_avg_diff.head(13)\n",
    "\n",
    "mte_moving_avg_diff.dropna(inplace=True)\n",
    "TestStationaryPlot(mte_moving_avg_diff)\n",
    "\n",
    "TestStationaryAdfuller(mte_moving_avg_diff)\n",
    "\n",
    "The rolling mean values appear to be varying slightly. The Test Statistic is smaller than the 10% 5%, and 1% of critical values. So, we can say with 99% confidence level that the dataset is a stationary series.\n",
    "\n",
    "## B). Exponentail weighted moving average\n",
    "\n",
    "Another technique is to take the ‘weighted moving average’ where more recent values are given a higher weight. The popular method to assign the waights is using the exponential weighted moving average. Where weights are assigned to all previous values with a decay factor.\n",
    "\n",
    "mte_exp_wighted_avg = pd.ewma(mte, halflife=12)\n",
    "plt.plot(mte)\n",
    "plt.plot(mte_exp_wighted_avg, color='red')\n",
    "plt.xticks(fontsize = 25)\n",
    "plt.yticks(fontsize = 25)\n",
    "plt.xlabel('Time (years)', fontsize = 25)\n",
    "plt.ylabel('CO2 Emission (MMT)', fontsize = 25)\n",
    "plt.title('CO2 emission from electric power generation', fontsize = 25)\n",
    "plt.show()\n",
    "\n",
    "mte_ewma_diff = mte - mte_exp_wighted_avg\n",
    "TestStationaryPlot(mte_ewma_diff)\n",
    "\n",
    "TestStationaryAdfuller(mte_ewma_diff)\n",
    "\n",
    "This time series has lesser variations in mean and standard deviation compared to the orginal ddataset. Also, the Test Statistic is smaller than the 5% and 10% critical value, which is better than the original case. There will be no missing values as all values from starting are given weights. So, it will work even with no previous values. In this case, we can say with 95% confidence level the series is a stationary series.\n",
    "\n",
    "## C) Eliminating trend and seasonality: Differencing\n",
    "\n",
    "One of the most common method of dealing with both trend and seasonality is differencing. In this technique, we take the difference of the original observation at a particular instant with that at the previous instant. This mostly works well to improve stationarity. First order differencing can be done as follows:\n",
    "\n",
    "mte_first_difference = mte - mte.shift(1)  \n",
    "TestStationaryPlot(mte_first_difference.dropna(inplace=False))\n",
    "\n",
    "TestStationaryAdfuller(mte_first_difference.dropna(inplace=False))\n",
    "\n",
    "The first difference improves the stationarity of the series significantly. Let us use also the ***seasonal difference*** to remove the seasonality of the data and see how that impacts stationarity of the data.\n",
    "\n",
    "mte_seasonal_difference = mte - mte.shift(12)  \n",
    "TestStationaryPlot(mte_seasonal_difference.dropna(inplace=False))\n",
    "TestStationaryAdfuller(mte_seasonal_difference.dropna(inplace=False))\n",
    "\n",
    "Compared to the original data the seasonal difference also improves the stationarity of the series. The next step is to take the first difference of the seasonal difference.\n",
    "\n",
    "mte_seasonal_first_difference = mte_first_difference - mte_first_difference.shift(12)  \n",
    "TestStationaryPlot(mte_seasonal_first_difference.dropna(inplace=False))\n",
    "\n",
    "TestStationaryAdfuller(mte_seasonal_first_difference.dropna(inplace=False))\n",
    "\n",
    "Now, if we look the Test Statistic and the p-value, taking the seasonal first difference has made our the time series dataset stationary. This differencing procedure could be repeated for the log values, but it didn’t make the dataset any more stationary.\n",
    "\n",
    "## D) Eliminating trend and seasonality: Decomposing\n",
    "\n",
    "In this technique, it statrating by modeling both trend and seasonality and removing them from the model.\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(mte)\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(mte, label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "\n",
    "Here we can see that the trend, seasonality are separated out from data and we can model the residuals. Lets check stationarity of residuals:\n",
    "\n",
    "mte_decompose = residual\n",
    "mte_decompose.dropna(inplace=True)\n",
    "TestStationaryPlot(mte_decompose)\n",
    "TestStationaryAdfuller(mte_decompose)\n",
    "\n",
    "# 6) Find optimal parameters and build SARIMA model\n",
    "\n",
    "When looking to fit time series dataset with seasonal ARIMA model, our first goal is to find the values of SARIMA(p,d,q)(P,D,Q)s that optimize our metric of interest. Before moving directly how to find the optimal values of the parameters let us see the two situations in stationarities: A strictly stationary series with no dependence among the values. This is the easy case wherein we can model the residuals as **white noise.** The second case being a series with significant dependence among values and needs statistical models like ARIMA to forecast future oucomes.\n",
    "\n",
    "**Auto-Regressive Integrated Moving Average (ARIMA)**: The ARIMA forecasting for a stationary time series is a linear funcion similar to linear regression. The predictors mainly depend on the parameters (p,d,q) of the ARIMA model:\n",
    "\n",
    "* Number of **Auto-Regressive (AR) terms (p)**: AR terms are just lags of dependent variable. For instance if p is 4, the predictors for x(t) will depend on x(t-1)….x(t-4).  This term allows us to incorporate the effect of past values into our model. This would be similar to stating that the weather is likely to be warm tomorrow if it has been warm the past 4 days.\n",
    "* Number of **Moving Average(MA) terms (q)**: MA terms are lagged forecast errors in prediction function. This term allows us to set the error of our model as a linear combination of the error values observed at previous time points in the past. For instance if q is 4, the predictors for x(t) will be e(t-1)….e(t-4) where e(i) is the difference between the moving average at ith instant and actual value.\n",
    "* Number of** Differences (d)**: These are the number of nonseasonal differences, i.e.,  if we took the first order difference. So either we can pass the first order difference variable and put d=0 or pass the original observed variable and put d=1. Both will generate same results. This term explains the number of past time points to subtract from the current value. This would be similar to stating that it is likely to be same temperature tomorrow if the difference in temperature in the last three days has been very small.\n",
    "\n",
    "# 6.1 Plot the ACF and PACF charts and find the optimal parameters\n",
    "\n",
    "* **Autocorrelation Function (ACF)**: It is a measure of the correlation between the the time series (ts) with a lagged version of itself. For instance at lag 4, ACF would compare series at time instant ‘t1’…’t2’ with series at instant ‘t1-4’…’t2-4’ (t1-4 and t2 being end points of the range).\n",
    "* **Partial Autocorrelation Function (PACF)**: This measures the correlation between the ts with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 4, it will check the correlation but remove the effects already explained by lags 1 to  3.\n",
    "\n",
    "Therefore, the next step will be determing the tuning parameters (p and q) of the model by looking at the autocorrelation and partial autocorrelation graphs.  The chart below provides a brief guide on how to read the autocorrelation and partial autocorrelation graphs inorder to select the parameters. \n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(mte_seasonal_first_difference.iloc[13:], lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(mte_seasonal_first_difference.iloc[13:], lags=40, ax=ax2)\n",
    "\n",
    "# 6.2 Grid search \n",
    "\n",
    "To find the optimal parameters for ARIMA models using the graphical method is not trivial and it is time consuming. We will select the optimal parameter values systematically using the grid search (hyperparameter optimization) method. The grid search iteratively explore different combinations of the parameters. For each combination of parameters, we will fit a new seasonal ARIMA model with the SARIMAX() function from the statsmodels module and assess its overall quality. Once we have explored the entire landscape of parameters, our optimal set of parameters will be the one that yields the best performance for our criteria of interest. Let's begin by generating the various combination of parameters that we wish to assess:\n",
    "\n",
    "p = d = q = range(0, 2) # Define the p, d and q parameters to take any value between 0 and 2\n",
    "pdq = list(itertools.product(p, d, q)) # Generate all different combinations of p, q and q triplets\n",
    "pdq_x_QDQs = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))] # Generate all different combinations of seasonal p, q and q triplets\n",
    "print('Examples of Seasonal ARIMA parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], pdq_x_QDQs[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], pdq_x_QDQs[2]))\n",
    "\n",
    "\n",
    "When evaluating and comparing statistical models fitted with different parameters, each can be ranked against one another based on how well it fits the data or its ability to accurately predict future data points. We will use the AIC (Akaike Information Criterion) value, which is conveniently returned with ARIMA models fitted using statsmodels. The AIC measures how well a model fits the data while taking into account the overall complexity of the model. A model that fits the data very well while using lots of features will be assigned a larger AIC score than a model that uses fewer features to achieve the same goodness-of-fit. The lowest AIC  refore, we are interested in finding the model that yields the lowest AIC value.\n",
    "\n",
    "The order argument specifies the (p, d, q) parameters, while the seasonal_order argument specifies the (P, D, Q, S) seasonal component of the Seasonal ARIMA model. After fitting each SARIMAX()model, the code prints out its respective AIC score.\n",
    "\n",
    "for param in pdq:\n",
    "    for seasonal_param in pdq_x_QDQs:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(mte,\n",
    "                                            order=param,\n",
    "                                            seasonal_order=seasonal_param,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "            results = mod.fit()\n",
    "            print('ARIMA{}x{} - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "SARIMAX(1, 1, 1)x(0, 1, 1, 12) yields the lowest AIC value of 2003.553. Therefore, we will consider this to be optimal option out of all the parameter combinations.W e have identified the set of parameters that produces the best fitting model to our time series data. We can proceed to analyze this particular model in more depth.\n",
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(mte, \n",
    "                                order=(1,1,1), \n",
    "                                seasonal_order=(0,1,1,12),   \n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results = mod.fit()\n",
    "print(results.summary())\n",
    "\n",
    "The coef column shows the weight (i.e. importance) of each feature and how each one impacts the time series. The P>|z| column informs us of the significance of each feature weight. Here, each weight has a p-value close to 0, so it is reasonable to include the features in our model.\n",
    "\n",
    "When fitting seasonal ARIMA models, it is important to run model diagnostics to ensure that none of the assumptions made by the model have been violated. First, we get a line plot of the residual errors, suggesting that there may still be some trend information not captured by the model.\n",
    "\n",
    "results.resid.plot()\n",
    "\n",
    "print(results.resid.describe())\n",
    "\n",
    "The figure displays the distribution of the residual errors. It shows a little bias in the prediction. Next, we get a density plot of the residual error values, suggesting the errors are Gaussian, but may not be centered on zero.\n",
    "\n",
    "results.resid.plot(kind='kde')\n",
    "\n",
    "The plot_diagnostics object allows us to quickly generate model diagnostics and investigate for any unusual behavior.\n",
    "\n",
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()\n",
    "\n",
    "Our primary concern is to ensure that the residuals of our model are uncorrelated and normally distributed with zero-mean. If the seasonal ARIMA model does not satisfy these properties, it is a good indication that it can be further improved.\n",
    "\n",
    "The model diagnostic suggests that the model residual is normally distributed based on the following:\n",
    "\n",
    "- In the top right plot, the red KDE line follows closely with the N(0,1) line. Where, N(0,1) is the standard notation for a normal distribution with mean 0 and standard deviation of 1. This is a good indication that the residuals are normally distributed. The forecast errors deviate somewhat from the straight line, indicating that the normal distribution is not a perfect model for the distribution of forecast errors, but it is not unreasonable.\n",
    "- The qq-plot on the bottom left shows that the ordered distribution of residuals (blue dots) follows the linear trend of the samples taken from a standard normal distribution. Again, this is a strong indication that the residuals are normally distributed.\n",
    "- The residuals over time (top left plot) don't display any obvious seasonality and appear to be white noise. This is confirmed by the autocorrelation (i.e. correlogram) plot on the bottom right, which shows that the time series residuals have low correlation with lagged versions of itself.\n",
    "\n",
    "Those observations lead us to conclude that our model produces a satisfactory fit that could help us understand our time series data and forecast future values.\n",
    "\n",
    "# 7) Validating prediction\n",
    "\n",
    "We have obtained a model for our time series that can now be used to produce forecasts. We start by comparing predicted values to real values of the time series, which will help us understand the accuracy of our forecast. The get_prediction() and conf_int() attributes allow us to obtain the values and associated confidence intervals for forecasts of the time series.\n",
    "\n",
    "pred = results.get_prediction(start = 480, end = 523, dynamic=False)\n",
    "pred_ci = pred.conf_int()\n",
    "pred_ci.head()\n",
    "\n",
    "The dynamic=False argument ensures that we produce one-step ahead forecasts, meaning that forecasts at each point are generated using the full history up to that point.\n",
    "\n",
    "We can plot the real and forecasted values of the CO2 emission time series to assess how well the model fits.\n",
    "\n",
    "ax = mte['1973':].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead forecast', alpha=.7)\n",
    "\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='r', alpha=.5)\n",
    "\n",
    "ax.set_xlabel('Time (years)')\n",
    "ax.set_ylabel('NG CO2 Emissions')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Overall, our forecasts align with the true values very well, showing an overall similar behavior.\n",
    "\n",
    "It is also useful to quantify the accuracy of our forecasts. We will use the MSE (Mean Squared Error), which summarizes the average error of our forecasts. For each predicted value, we compute its distance to the true value and square the result. The results need to be squared so that positive/negative differences do not cancel each other out.\n",
    "\n",
    "mte_forecast = pred.predicted_mean\n",
    "mte_truth = mte['2013-01-31':]\n",
    "\n",
    "# Compute the mean square error\n",
    "mse = ((mte_forecast - mte_truth) ** 2).mean()\n",
    "print('The Mean Squared Error (MSE) of the forecast is {}'.format(round(mse, 2)))\n",
    "print('The Root Mean Square Error (RMSE) of the forcast: {:.4f}'\n",
    "      .format(np.sqrt(sum((mte_forecast-mte_truth)**2)/len(mte_forecast))))\n",
    "\n",
    "mte_pred_concat = pd.concat([mte_truth, mte_forecast])\n",
    "\n",
    "The goal of developing the model is to get a good quality predictive power using dynamic forecast. That is, we use information from the time series up to a certain point, and after that, forecasts are generated using values from previous forecasted time points as follows:\n",
    "\n",
    "pred_dynamic = results.get_prediction(start=pd.to_datetime('2013-01-31'), dynamic=True, full_results=True)\n",
    "pred_dynamic_ci = pred_dynamic.conf_int()\n",
    "\n",
    "From, plotting the observed and forecasted values of the time series, we see that the overall forecasts are accurate even when we use the dynamic forecast. All forecasted values (red line) match closely to the orginal observed (blue line) data, and are well within the confidence intervals of our forecast.\n",
    "\n",
    "ax = mte['1973':].plot(label='observed', figsize=(20, 15))\n",
    "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(pred_dynamic_ci.index,\n",
    "                pred_dynamic_ci.iloc[:, 0],\n",
    "                pred_dynamic_ci.iloc[:, 1], \n",
    "                color='r', \n",
    "                alpha=.3)\n",
    "\n",
    "ax.fill_betweenx(ax.get_ylim(), \n",
    "                 pd.to_datetime('2013-01-31'), \n",
    "                 mte.index[-1],\n",
    "                 alpha=.1, zorder=-1)\n",
    "\n",
    "ax.set_xlabel('Time (years)')\n",
    "ax.set_ylabel('CO2 Emissions')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Extract the predicted and true values of our time series\n",
    "mte_forecast = pred_dynamic.predicted_mean\n",
    "mte_orginal = mte['2013-01-31':]\n",
    "\n",
    "# Compute the mean square error\n",
    "mse = ((mte_forecast - mte_orginal) ** 2).mean()\n",
    "print('The Mean Squared Error (MSE) of the forecast is {}'.format(round(mse, 2)))\n",
    "print('The Root Mean Square Error (RMSE) of the forcast: {:.4f}'\n",
    "      .format(np.sqrt(sum((mte_forecast-mte_orginal)**2)/len(mte_forecast))))\n",
    "\n",
    "# 8) Forecasting\n",
    "\n",
    "# Get forecast of 10 years or 120 months steps ahead in future\n",
    "forecast = results.get_forecast(steps= 120)\n",
    "# Get confidence intervals of forecasts\n",
    "forecast_ci = forecast.conf_int()\n",
    "forecast_ci.head()\n",
    "\n",
    "We can use the output of this code to plot the time series and forecasts of its future values.\n",
    "\n",
    "ax = mte.plot(label='observed', figsize=(20, 15))\n",
    "forecast.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "ax.fill_between(forecast_ci.index,\n",
    "                forecast_ci.iloc[:, 0],\n",
    "                forecast_ci.iloc[:, 1], color='g', alpha=.4)\n",
    "ax.set_xlabel('Time (year)')\n",
    "ax.set_ylabel('NG CO2 Emission level')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "Both the forecast and associated confidence interval that we have generated can now be used to further explore and understand the time series. The forecast shows that the CO2 emission from natural gas power generation is expected to continue increasing.\n",
    "\n",
    "# 9) Conclusion\n",
    "\n",
    "In this notebook, I have explored how to retrieve CSV dataset, how to transform the dataset into times series, testing if the time series is stationary or not using graphical and Dickey-Fuller test statistic methods, how to transform time series to stationary, how to find optimal parameters to build SARIMA model using grid search method, diagnosing time series prediction, validating the predictive power, forecasting 10 year future CO2 emission from power generation using natural gas.\n",
    "\n",
    "Future work: developing a time series model of natural gas forecasing\n",
    "\n",
    "\n",
    "Suggestion, comments and questions are welcome! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
